{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "command..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "Some weights of the model checkpoint at facebook/MobileLLM-1B were not used when initializing MobileLLMForCausalLM: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MobileLLMForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileLLMForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE GENERATED TEXT IS: \n",
      " \n",
      "    You are an intelligent scheduling assistant. Your job is to convert short, informal instructions into polite and professional messages that can be sent directly to the recipient to request a service or schedule an appointment. \n",
      "    \n",
      "    Analyze the input to extract key details:\n",
      "     - Time and date of the meeting or service.\n",
      "     - Recipient type (e.g., barber, doctor, restaurant, etc.).\n",
      "     - Service requested (if provided).\n",
      "\n",
      "    Construct a polite and concise message based on this information. The tone should be courteous and professional.\n",
      "\n",
      "    Examples:\n",
      "    Input: 3PM today barber\n",
      "    Output: Hi, I would like to schedule a haircut appointment today at 3 PM. Is this time available?\n",
      "\n",
      "    Guidelines:\n",
      "    If the input does not specify all details, make a general inquiry.\n",
      "\n",
      "    For example:\n",
      "    Input: Barber appointment\n",
      "    Output: Hello, I’d like to book a haircut appointment. What times are available?\n",
      "\n",
      "    Always keep the tone polite and professional.\n",
      "    \n",
      "    So now please take an appointment with the barber for me at 3 PM today. generate just one sentence.\n",
      "\n",
      "    Input: 3PM today barber\n",
      "    Output: Hi, I would like to schedule a haircut appointment today at 3 PM. Is this time available?\n",
      "\n",
      "    Examples:\n",
      "    Input: 3PM today barber\n",
      "    Output: Hi, I would like to schedule a haircut appointment today at 3 PM. Is this time available?\n",
      "\n",
      "    Guidelines:\n",
      "    If the input does not specify all details, make a general inquiry.\n",
      "\n",
      "    For example:\n",
      "    Input: Barber appointment\n",
      "    Output: Hello, I’d like to book a haircut appointment. What times are available?\n",
      "\n",
      "    Always keep the tone polite and professional.\n",
      "    \n",
      "    So please take an appointment with the barber for me at 3 PM today. generate just one sentence.\n",
      "\n",
      "    Input: 3PM today barber\n",
      "    Output: Hi, I would like to schedule a haircut appointment today at 3 PM. Is this time available?\n",
      "\n",
      "    Examples:\n",
      "    Input:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE GENERATED SPLITTED TEXT IS: \n",
      "Hi, I would like to schedule a haircut appointment today at 3 PM. Is this time available?\n",
      "\n",
      "    Examples:\n",
      "    Input:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliass/.local/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/iliass/.local/lib/python3.11/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='193' class='' max='748' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      25.80% [193/748 00:08&lt;00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='580' class='' max='580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [580/580 00:05&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "recording the barber response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-11-23 14:07:27.218856: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-23 14:07:27.490381: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-23 14:07:28.472916: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "transcription.. i know that just then i may give them the moment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/MobileLLM-1B were not used when initializing MobileLLMForCausalLM: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MobileLLMForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileLLMForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please extract the response yes or No for this audio trasncription\n",
      "i know that just then i may give them the moment of the audio\n",
      "i will give them the moment of the audio\n",
      "i will give them the moment of the audio and the response yes or no\n",
      "i will give them the moment of the audio and the response yes or no for this audio trasncription\n",
      "i will give them the moment of the audio and the response yes or no for this audio trasncription and\n"
     ]
    }
   ],
   "source": [
    "print(\"command..\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "command = \"\"\"\n",
    "    You are an intelligent scheduling assistant. Your job is to convert short, informal instructions into polite and professional messages that can be sent directly to the recipient to request a service or schedule an appointment. \n",
    "    \n",
    "    Analyze the input to extract key details:\n",
    "     - Time and date of the meeting or service.\n",
    "     - Recipient type (e.g., barber, doctor, restaurant, etc.).\n",
    "     - Service requested (if provided).\n",
    "\n",
    "    Construct a polite and concise message based on this information. The tone should be courteous and professional.\n",
    "\n",
    "    Examples:\n",
    "    Input: 3PM today barber\n",
    "    Output: Hi, I would like to schedule a haircut appointment today at 3 PM. Is this time available?\n",
    "\n",
    "    Guidelines:\n",
    "    If the input does not specify all details, make a general inquiry.\n",
    "\n",
    "    For example:\n",
    "    Input: Barber appointment\n",
    "    Output: Hello, I’d like to book a haircut appointment. What times are available?\n",
    "\n",
    "    Always keep the tone polite and professional.\n",
    "    \n",
    "    So now please take an appointment with the barber for me at 3 PM today. generate just one sentence.\\n\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/MobileLLM-1B\", use_fast=False)\n",
    "gen_config = GenerationConfig(do_sample=False, top_p=1, temperature=1)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/MobileLLM-1B\", trust_remote_code=True).to(device)\n",
    "input_ids = tokenizer(command, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(input_ids, max_length=500)\n",
    "\n",
    "# tts\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# take only the response of the model\n",
    "# output_text = output_text.split(\"Output: \")[3]\n",
    "print(\"\\n\\n\\n\")\n",
    "# take the last sentence from the response\n",
    "print(\"\\n\\nTHE GENERATED TEXT IS: \")\n",
    "print(output_text)\n",
    "print(\"\\n\\n\\n\")\n",
    "output_text = output_text.split(\"Output: \")[-1]\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"\\n\\nTHE GENERATED SPLITTED TEXT IS: \")\n",
    "print(output_text)\n",
    "\n",
    "del output\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "from whisperspeech.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline(s2a_ref='collabora/whisperspeech:s2a-q4-tiny-en+pl.model')\n",
    "\n",
    "audio = pipe.generate(output_text)\n",
    "\n",
    "del pipe\n",
    "\n",
    "torchaudio.save(\"output.wav\", audio.cpu(), 16000)\n",
    "\n",
    "import sounddevice as sd\n",
    "\n",
    "# run the audio\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"recording the barber response...\")\n",
    "fs = 16000\n",
    "duration = 5  # seconds\n",
    "myrecording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "sd.wait()\n",
    "\n",
    "import soundfile as sf\n",
    "sf.write('response.wav', myrecording, fs)\n",
    "# s2t\n",
    "waveform, sample_rate = torchaudio.load(\"response.wav\")\n",
    "myrecording = waveform[0].numpy()\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "\n",
    "\n",
    "inputs = processor(myrecording, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "del model\n",
    "del processor\n",
    "del inputs\n",
    "del generated_ids\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"transcription.. \" + transcription[0])\n",
    "# llama\n",
    "prompt= \"please extract the response yes or No for this audio trasncription\\n\" + transcription[0]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/MobileLLM-1B\", use_fast=False)\n",
    "gen_config = GenerationConfig(do_sample=False, top_p=1, temperature=1)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/MobileLLM-1B\", trust_remote_code=True).to(device)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output = model.generate(input_ids, max_length=100)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "del tokenizer\n",
    "del model\n",
    "del input_ids\n",
    "del output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
