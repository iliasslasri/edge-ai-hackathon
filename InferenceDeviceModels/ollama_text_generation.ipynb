{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wandrilleflamantphospho/Documents/edge-ai-hackathon/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import GPT2Tokenizer\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama3() -> Ollama:\n",
    "    \"\"\"\n",
    "    Load the LLAMA3 model from OLLAMA.\n",
    "    \"\"\"\n",
    "\n",
    "    ollama_client = Ollama(base_url=\"http://localhost:11434\", model=\"llama3\")\n",
    "\n",
    "    system_message = (\n",
    "        \"\"\"You are an intelligent scheduling assistant. Your job is to convert short, informal instructions into polite and professional messages that can be sent directly to the recipient to request a service or schedule an appointment. \n",
    "    \n",
    "    Analyze the input to extract key details:\n",
    "     - Time and date of the meeting or service.\n",
    "     - Recipient type (e.g., barber, doctor, restaurant, etc.).\n",
    "     - Service requested (if provided).\n",
    "\n",
    "    Construct a polite and concise message based on this information. The tone should be courteous and professional.\n",
    "\n",
    "    Examples:\n",
    "    Input: 3PM today barber\n",
    "    Output: Hi, I would like to schedule a haircut appointment today at 3 PM. Is this time available?\n",
    "    Input: 5 PM tomorrow Italian restaurant for 2\n",
    "    Output: Hello, I’d like to reserve a table for two at your Italian restaurant tomorrow at 5 PM. Is that possible?\n",
    "    Input: 9 AM dentist next Tuesday\n",
    "    Output: Good morning, I’d like to schedule a dental appointment next Tuesday at 9 AM. Does that work for your schedule?\n",
    "\n",
    "    Guidelines:\n",
    "    If the input does not specify all details, make a general inquiry.\n",
    "\n",
    "    For example:\n",
    "    Input: Barber appointment\n",
    "    Output: Hello, I’d like to book a haircut appointment. What times are available?\n",
    "    Always keep the tone polite and professional.\"\"\"\n",
    "    )\n",
    "\n",
    "    SystemMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=[],  # No input variables for the system message\n",
    "            template=system_message,\n",
    "        )\n",
    "    )\n",
    "    HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(input_variables=[\"input\"], template=\"{input}\")\n",
    "    )\n",
    "\n",
    "    return ollama_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_ollama(\n",
    "    input_text: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary for a given text using the OLLAMA model.\n",
    "    \"\"\"\n",
    "\n",
    "    ollama_model = load_llama3()\n",
    "    prompt = f\"\\n\\nInput: {input_text}\\n\\nConstruct a polite and concise message based on this information.\\n\\nOutput:\"\n",
    "\n",
    "    response = ollama_model(prompt)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ht/gnfxwh354lgdzms1j7hx2hp80000gn/T/ipykernel_39793/2672635637.py:6: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  ollama_client = Ollama(base_url=\"http://localhost:11434\", model=\"llama3\")\n",
      "/var/folders/ht/gnfxwh354lgdzms1j7hx2hp80000gn/T/ipykernel_39793/3569868309.py:11: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = ollama_model(prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Hey! Would you like to grab a drink sometime between 2am and 3pm tomorrow? I\\'d love to catch up with you then. Let me know if that works for you!\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer_ollama(\"Between 2AM and 3PM tomorrow, Drink with Nico.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a polite and concise message:\\n\\n\"Hi, I\\'d like to make a reservation at your Italian restaurant for an upcoming lunch. We\\'re a group of 8 people and would like to dine with you on [insert date]. Could you please let me know if you have availability at that time? Thank you!\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer_ollama(\"lunch italian restaurant 8 people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\")\n",
    "\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "# You can replace this embedding with your own as well.\n",
    "\n",
    "speech = synthesiser(generate_answer_ollama(\"Barber next monday afternon.\"), forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "\n",
    "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
